{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Day 4: Matrix Multiplication\n",
    "\n",
    "Welcome to Day 4! Today we tackle one of the most fundamental and powerful operations in linear algebra: **matrix multiplication**. This operation is at the heart of nearly every machine learning algorithm, from neural networks to principal component analysis.\n",
    "\n",
    "Unlike matrix addition and scalar multiplication, matrix multiplication is *not* element-wise. It involves a specific row-by-column multiplication and summation process.\n",
    "\n",
    "## Objectives for Today:\n",
    "- Understand the **rules** for matrix multiplication, especially dimension compatibility.\n",
    "- Learn how to perform matrix multiplication using NumPy (`np.dot` and `@`).\n",
    "- Understand the result's dimensions based on input matrix dimensions.\n",
    "- Explore **matrix-vector multiplication** as a special case.\n",
    "- Understand that matrix multiplication is generally **non-commutative**.\n",
    "- Connect matrix multiplication to its crucial role in Machine Learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np"
   ],
   "metadata": {
    "scrolled": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Rules of Matrix Multiplication (A @ B)\n",
    "\n",
    "### Concept\n",
    "For two matrices $A$ and $B$ to be multiplied to form $C = A B$:\n",
    "\n",
    "1.  **Dimension Compatibility:** The number of **columns** in the first matrix ($A$) must be equal to the number of **rows** in the second matrix ($B$).\n",
    "    If $A$ is an `m x k` matrix and $B$ is a `k x n` matrix, then they can be multiplied.\n",
    "\n",
    "2.  **Resulting Dimensions:** The resulting matrix $C$ will have dimensions `m x n` (number of rows from $A$ by number of columns from $B$).\n",
    "\n",
    "    $$ (m \\times k) \\cdot (k \\times n) = (m \\times n) $$\n",
    "\n",
    "3.  **Element Calculation:** Each element $c_{ij}$ in the resulting matrix $C$ is computed by taking the **dot product** of the $i$-th row of matrix $A$ and the $j$-th column of matrix $B$.\n",
    "\n",
    "    Example: For $A = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}$ and $B = \\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}$,\n",
    "    $C = A B = \\begin{pmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\ a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\end{pmatrix}$\n",
    "\n",
    "### NumPy Practice\n",
    "NumPy provides `np.dot()` and the `@` operator (introduced in Python 3.5) for matrix multiplication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Example 1: Multiplying a 2x3 matrix by a 3x2 matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [7, 8],\n",
    "    [9, 10],\n",
    "    [11, 12]\n",
    "])\n",
    "\n",
    "print(\"Matrix A (2x3):\\n\", A)\n",
    "print(\"Shape of A:\", A.shape)\n",
    "print(\"\\nMatrix B (3x2):\\n\", B)\n",
    "print(\"Shape of B:\", B.shape)\n",
    "\n",
    "# Using np.dot()\n",
    "C_dot = np.dot(A, B)\n",
    "print(\"\\nResult C = A . B (using np.dot):\\n\", C_dot)\n",
    "print(\"Shape of C:\", C_dot.shape) # Should be (2, 2)\n",
    "\n",
    "# Using the @ operator\n",
    "C_at = A @ B\n",
    "print(\"\\nResult C = A @ B (using @ operator):\\n\", C_at)\n",
    "print(\"Shape of C:\", C_at.shape) # Should be (2, 2)\n",
    "\n",
    "# Example 2: Incompatible dimensions\n",
    "D = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "E = np.array([\n",
    "    [5, 6],\n",
    "    [7, 8],\n",
    "    [9, 10]\n",
    "])\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(\"Matrix D (2x2):\\n\", D)\n",
    "print(\"Shape of D:\", D.shape)\n",
    "print(\"\\nMatrix E (3x2):\\n\", E)\n",
    "print(\"Shape of E:\", E.shape)\n",
    "\n",
    "try:\n",
    "    D @ E # This will cause an error because (2x2) @ (3x2) is not compatible\n",
    "except ValueError as e:\n",
    "    print(\"\\nError when trying D @ E (2x2 @ 3x2):\", e)\n",
    "    print(\"Reason: Number of columns in D (2) does not match number of rows in E (3).\")\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 1: Basic Matrix Multiplication**\n",
    "\n",
    "Given the following matrices:\n",
    "\n",
    "```python\n",
    "X = np.array([\n",
    "   ,\n",
    "   ,\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "   ,\n",
    "   ,\n",
    "   \n",
    "])\n",
    "```\n",
    "\n",
    "1.  Print `X`, `Y`, and their shapes.\n",
    "2.  Predict the shape of the result `Z = X @ Y`.\n",
    "3.  Calculate `Z` using the `@` operator.\n",
    "4.  Print `Z` and its shape, verifying your prediction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 1 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 1\n",
    "X = np.array([\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0],\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [1, 2],\n",
    "    [0, 1],\n",
    "    [3, 0]\n",
    "])\n",
    "\n",
    "print(\"Matrix X ({}x{}):\\n{}\".format(*X.shape, X))\n",
    "print(\"Matrix Y ({}x{}):\\n{}\".format(*Y.shape, Y))\n",
    "\n",
    "print(\"\\nPrediction for Z = X @ Y shape: (rows of X) x (columns of Y) = {}x{}\".format(X.shape[0], Y.shape[1]))\n",
    "\n",
    "Z = X @ Y\n",
    "\n",
    "print(\"\\nResult Z ({}x{}):\\n{}\".format(*Z.shape, Z))\n",
    "print(\"Shape of Z:\", Z.shape)\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Matrix-Vector Multiplication\n",
    "\n",
    "### Concept\n",
    "Matrix-vector multiplication is a special case of matrix multiplication where the second matrix ($B$) is a column vector.\n",
    "\n",
    "If $A$ is an `m x n` matrix and $v$ is an `n`-dimensional vector (which can be treated as an `n x 1` column matrix), then the result $b = A v$ is an `m`-dimensional vector (or `m x 1` column matrix).\n",
    "\n",
    "Each element of the resulting vector $b$ is the dot product of a row of $A$ with the vector $v$.\n",
    "\n",
    "### NumPy Practice\n",
    "NumPy handles this naturally with `np.dot()` or `@`. A 1D NumPy array `(n,)` is correctly interpreted as a column vector `(n,1)` for this operation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Example: Transformation of a vector\n",
    "T = np.array([\n",
    "    [2, 0],\n",
    "    [0, 3]\n",
    "])\n",
    "\n",
    "v = np.array([1, 1])\n",
    "\n",
    "print(\"Transformation Matrix T (2x2):\\n\", T)\n",
    "print(\"Shape of T:\", T.shape)\n",
    "print(\"\\nVector v (2,):\", v)\n",
    "print(\"Shape of v:\", v.shape)\n",
    "\n",
    "transformed_v = T @ v\n",
    "\n",
    "print(\"\\nTransformed Vector (T @ v):\", transformed_v)\n",
    "print(\"Shape of Transformed Vector:\", transformed_v.shape) # Still (2,)\n",
    "\n",
    "# Let's visualize this transformation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Original Vector v')\n",
    "plt.quiver(0, 0, transformed_v[0], transformed_v[1], angles='xy', scale_units='xy', scale=1, color='red', label='Transformed Vector T@v')\n",
    "\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.grid()\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Matrix-Vector Transformation\")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2: Matrix-Vector Multiplication (Neural Network Application)**\n",
    "\n",
    "In a simple neural network, the output of a layer is often calculated by multiplying the input vector by a weight matrix and then adding a bias. Let's simplify and just consider the multiplication part.\n",
    "\n",
    "Given an `input_features` vector and a `weights` matrix:\n",
    "\n",
    "```python\n",
    "input_features = np.array([0.5, 1.2, -0.3]) # Example: [feature1, feature2, feature3]\n",
    "\n",
    "weights = np.array([\n",
    "    [0.1, 0.4],\n",
    "    [0.7, 0.2],\n",
    "    [0.3, 0.9]\n",
    "]) # Example: (number of input features) x (number of neurons in next layer)\n",
    "```\n",
    "\n",
    "1.  Print `input_features`, `weights`, and their shapes.\n",
    "2.  Predict the shape of the result `output_activations = input_features @ weights`.\n",
    "3.  Calculate `output_activations`.\n",
    "4.  Print the `output_activations` vector and its shape.\n",
    "5.  Explain in your own words what each element in `output_activations` conceptually represents in this simple neural network context."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 2 here\n",
    "\n",
    "# Your explanation here:\n",
    "# (Type your answer here as a Python comment or in a markdown cell below)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 2\n",
    "input_features = np.array([0.5, 1.2, -0.3])\n",
    "\n",
    "weights = np.array([\n",
    "    [0.1, 0.4],\n",
    "    [0.7, 0.2],\n",
    "    [0.3, 0.9]\n",
    "])\n",
    "\n",
    "print(\"Input Features Vector ({}):\\n{}\".format(input_features.shape, input_features))\n",
    "print(\"Weights Matrix ({}x{}):\\n{}\".format(*weights.shape, weights))\n",
    "\n",
    "print(\"\\nPrediction for output_activations shape: (rows of input_features is effectively 1) x (columns of weights) = (1, {}) or just ({},)\".format(weights.shape[1], weights.shape[1]))\n",
    "\n",
    "output_activations = input_features @ weights\n",
    "\n",
    "print(\"\\nOutput Activations Vector ({}):\\n{}\".format(output_activations.shape, output_activations))\n",
    "print(\"Shape of Output Activations:\", output_activations.shape)\n",
    "\n",
    "print(\"\\nExplanation:\\n\")\n",
    "print(\"In this context, each element in the `output_activations` vector represents the 'activation' or 'weighted sum'\\n\",\n",
    "      \"for a particular neuron in the next layer of the neural network. For example, `output_activations[0]`\\n\",\n",
    "      \"is the weighted sum of `input_features` where the weights are taken from the first column of the `weights` matrix.\\n\",\n",
    "      \"This is a fundamental step before applying an activation function (like ReLU or Sigmoid) to introduce non-linearity.\")\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Properties: Matrix Multiplication is Generally Non-Commutative\n",
    "\n",
    "### Concept\n",
    "One crucial property of matrix multiplication is that it is generally **not commutative**. This means that for two matrices $A$ and $B$, $A B \\neq B A$. In fact, $B A$ might not even be defined if the dimensions don't match (as we saw in an earlier example).\n",
    "\n",
    "Even for square matrices where both $A B$ and $B A$ are defined and have the same dimensions, the results are almost always different.\n",
    "\n",
    "### NumPy Practice\n",
    "Let's demonstrate this with an example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "matrix_A = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "\n",
    "matrix_B = np.array([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "\n",
    "print(\"Matrix A:\\n\", matrix_A)\n",
    "print(\"\\nMatrix B:\\n\", matrix_B)\n",
    "\n",
    "# Calculate A @ B\n",
    "AB = matrix_A @ matrix_B\n",
    "print(\"\\nResult of A @ B:\\n\", AB)\n",
    "\n",
    "# Calculate B @ A\n",
    "BA = matrix_B @ matrix_A\n",
    "print(\"\\nResult of B @ A:\\n\", BA)\n",
    "\n",
    "# Check if they are equal\n",
    "print(\"\\nAre A @ B and B @ A equal?:\", np.array_equal(AB, BA))\n",
    "\n",
    "if not np.array_equal(AB, BA):\n",
    "    print(\"This confirms that matrix multiplication is generally not commutative.\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 3: Demonstrating Non-Commutativity**\n",
    "\n",
    "1.  Create two new distinct 3x3 square matrices, `M_alpha` and `M_beta`, with different integer values.\n",
    "2.  Calculate `product1 = M_alpha @ M_beta`.\n",
    "3.  Calculate `product2 = M_beta @ M_alpha`.\n",
    "4.  Print `M_alpha`, `M_beta`, `product1`, and `product2`.\n",
    "5.  Use `np.array_equal()` to explicitly check if `product1` and `product2` are identical and print the result.\n",
    "6.  Briefly comment on the implications of non-commutativity in real-world applications (e.g., sequences of transformations)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 3 here\n",
    "\n",
    "# Your comment here:\n",
    "# (Type your answer here as a Python comment or in a markdown cell below)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 3\n",
    "M_alpha = np.array([\n",
    "    [1, 2, 3],\n",
    "    [0, 1, 4],\n",
    "    [5, 6, 7]\n",
    "])\n",
    "\n",
    "M_beta = np.array([\n",
    "    [8, 1, 2],\n",
    "    [3, 0, 5],\n",
    "    [6, 7, 0]\n",
    "])\n",
    "\n",
    "print(\"Matrix M_alpha:\\n\", M_alpha)\n",
    "print(\"\\nMatrix M_beta:\\n\", M_beta)\n",
    "\n",
    "product1 = M_alpha @ M_beta\n",
    "print(\"\\nResult of M_alpha @ M_beta (Product 1):\\n\", product1)\n",
    "\n",
    "product2 = M_beta @ M_alpha\n",
    "print(\"\\nResult of M_beta @ M_alpha (Product 2):\\n\", product2)\n",
    "\n",
    "are_equal = np.array_equal(product1, product2)\n",
    "print(\"\\nAre Product 1 and Product 2 equal?:\", are_equal)\n",
    "\n",
    "print(\"\\nImplications of Non-Commutativity:\\n\")\n",
    "print(\"Non-commutativity means that the order of matrix multiplications matters significantly.\\n\",\n",
    "      \"In real-world applications, especially in computer graphics, robotics, or sequential data processing,\\n\",\n",
    "      \"a sequence of transformations (like rotations, scaling, translations represented by matrices)\\n\",\n",
    "      \"will produce different final results depending on the order in which they are applied. For example,\\n\",\n",
    "      \"rotating an object then moving it is generally not the same as moving it then rotating it.\")\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Day 4 Summary and ML Connection\n",
    "\n",
    "Today was a deep dive into **matrix multiplication**, a concept that is absolutely fundamental to machine learning:\n",
    "\n",
    "-   We learned the strict rules for matrix multiplication: the inner dimensions must match, and the outer dimensions determine the result's shape.\n",
    "-   **NumPy's `@` operator** (or `np.dot()`) makes matrix multiplication efficient and easy in Python.\n",
    "-   **Matrix-vector multiplication** is a special case, crucial for applying transformations to individual data points or calculating neuron activations in neural networks.\n",
    "-   The **non-commutative** nature of matrix multiplication means the order of operations matters. This is vital when chaining transformations or processing sequential data.\n",
    "\n",
    "**ML Connections Recap:**\n",
    "-   **Neural Networks:** Matrix multiplication forms the core of how information flows through layers. `Output = Input @ Weights + Bias` is the fundamental equation for a linear layer.\n",
    "-   **Data Transformations:** Scaling, rotation, and other linear transformations of data are all represented and performed via matrix multiplication.\n",
    "-   **Feature Engineering:** Creating new features from existing ones can sometimes involve matrix products.\n",
    "-   **Linear Regression:** The closed-form solution for ordinary least squares involves matrix multiplication and inverses (which we'll see soon!).\n",
    "\n",
    "Understanding matrix multiplication deeply is a cornerstone for comprehending how most modern machine learning models operate. Keep practicing, and you'll find this operation becomes second nature!"
   ],
   "metadata": {}
  }
 ]
}
