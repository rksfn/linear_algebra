{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b1a8c2",
   "metadata": {},
   "source": [
    "# Day 7: Eigenvalues and Eigenvectors\n",
    "\n",
    "Welcome to Day 7! Today, we explore one of the most important concepts in linear algebra for machine learning: **eigenvalues** and **eigenvectors**. These concepts are at the heart of dimensionality reduction techniques like Principal Component Analysis (PCA) and help us understand the underlying structure of data.\n",
    "\n",
    "## Objectives for Today:\n",
    "- Understand the definition of eigenvalues and eigenvectors.\n",
    "- Grasp their geometric interpretation (vectors that only scale under a transformation).\n",
    "- Learn how to calculate eigenvalues and eigenvectors using NumPy.\n",
    "- Verify the fundamental eigenvector equation: `Av = λv`.\n",
    "- Connect these concepts to their application in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e0f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2d0e4",
   "metadata": {},
   "source": [
    "## 1. What are Eigenvalues and Eigenvectors?\n",
    "\n",
    "For a given square matrix `A` (which represents a linear transformation), an **eigenvector** is a special non-zero vector `v` that, when the matrix `A` is multiplied by it, yields a new vector that is simply a scaled version of the original vector `v`.\n",
    "\n",
    "The scalar used to scale the eigenvector is the **eigenvalue**, denoted by `λ` (lambda).\n",
    "\n",
    "This relationship is captured by the fundamental equation:\n",
    "\n",
    "### **`Av = λv`**\n",
    "\n",
    "Where:\n",
    "-   `A` is an `n x n` square matrix (the transformation).\n",
    "-   `v` is an `n x 1` non-zero column vector (the **eigenvector**).\n",
    "-   `λ` is a scalar (the **eigenvalue**).\n",
    "\n",
    "### Geometric Interpretation\n",
    "Imagine a transformation `A` that rotates and stretches space. Most vectors will change their direction after the transformation. However, **eigenvectors** are special because they **do not change their direction** (or point in the exact opposite direction). They are only stretched or shrunk by a factor of their corresponding eigenvalue `λ`.\n",
    "\n",
    "- If `λ > 1`, the eigenvector is stretched.\n",
    "- If `0 < λ < 1`, the eigenvector is shrunk.\n",
    "- If `λ < 0`, the eigenvector is flipped and then scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3e1f5",
   "metadata": {},
   "source": [
    "## 2. Eigen-decomposition with NumPy\n",
    "\n",
    "Calculating eigenvalues and eigenvectors by hand involves solving the characteristic equation `det(A - λI) = 0`, which can be complex. Fortunately, NumPy provides a convenient function: `np.linalg.eig()`.\n",
    "\n",
    "This function takes a square matrix `A` and returns a tuple containing:\n",
    "1.  A 1D array of eigenvalues.\n",
    "2.  A 2D array where each **column** is a corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e4f6g7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 4]\n",
      " [2 3]]\n",
      "\n",
      "---\n",
      "\n",
      "Eigenvalues:\n",
      " [-1.  5.]\n",
      "\n",
      "Eigenvectors (each column is an eigenvector):\n",
      " [[-0.89442719 -0.70710678]\n",
      " [ 0.4472136  -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Define a 2x2 matrix\n",
    "A = np.array([[1, 4],\n",
    "              [2, 3]])\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nEigenvectors (each column is an eigenvector):\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7g8h9",
   "metadata": {},
   "source": [
    "### **Exercise 1: Find Eigenvalues and Eigenvectors**\n",
    "\n",
    "1.  Create the following matrix `B`:\n",
    "    ```\n",
    "    [[ 2, -1],\n",
    "     [-2,  3]]\n",
    "    ```\n",
    "2.  Use `np.linalg.eig()` to find its eigenvalues and eigenvectors.\n",
    "3.  Print both the eigenvalues and the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7g8h9i0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "g8h9i0j1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B:\n",
      " [[ 2 -1]\n",
      " [-2  3]]\n",
      "\n",
      "Eigenvalues of B:\n",
      " [1. 4.]\n",
      "\n",
      "Eigenvectors of B:\n",
      " [[-0.70710678  0.4472136 ]\n",
      " [-0.70710678 -0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "B = np.array([[2, -1],\n",
    "              [-2, 3]])\n",
    "\n",
    "eig_vals_B, eig_vecs_B = np.linalg.eig(B)\n",
    "\n",
    "print(\"Matrix B:\\n\", B)\n",
    "print(\"\\nEigenvalues of B:\\n\", eig_vals_B)\n",
    "print(\"\\nEigenvectors of B:\\n\", eig_vecs_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9i0j1k2",
   "metadata": {},
   "source": [
    "## 3. Verifying the Eigenvector Equation\n",
    "\n",
    "Let's confirm the `Av = λv` relationship using the results from our first example.\n",
    "We will check for the first eigenvalue and its corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "i0j1k2l3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Eigenvalue (λ1): -1.0\n",
      "First Eigenvector (v1):\n",
      " [-0.89442719  0.4472136 ]\n",
      "\n",
      "---\n",
      "Check 1: A @ v1\n",
      " [ 0.89442719 -0.4472136 ]\n",
      "\n",
      "Check 2: λ1 * v1\n",
      " [ 0.89442719 -0.4472136 ]\n",
      "\n",
      "Are they close? True\n"
     ]
    }
   ],
   "source": [
    "# Get the first eigenvalue and eigenvector\n",
    "lambda1 = eigenvalues[0]\n",
    "v1 = eigenvectors[:, 0] # First column\n",
    "\n",
    "print(\"First Eigenvalue (λ1):\", np.round(lambda1, 2))\n",
    "print(\"First Eigenvector (v1):\\n\", v1)\n",
    "\n",
    "# Calculate A * v1\n",
    "Av1 = A @ v1\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"Check 1: A @ v1\\n\", Av1)\n",
    "\n",
    "# Calculate λ1 * v1\n",
    "lambda1_v1 = lambda1 * v1\n",
    "print(\"\\nCheck 2: λ1 * v1\\n\", lambda1_v1)\n",
    "\n",
    "# Use np.allclose() to check for equality with a tolerance for floating point errors\n",
    "print(\"\\nAre they close?\", np.allclose(Av1, lambda1_v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j1k2l3m4",
   "metadata": {},
   "source": [
    "### **Exercise 2: Verify the Relationship**\n",
    "\n",
    "1.  Take the **second** eigenvalue (`eig_vals_B[1]`) and eigenvector (`eig_vecs_B[:, 1]`) you calculated for matrix `B` in Exercise 1.\n",
    "2.  Verify that `B @ v` is approximately equal to `λ * v`.\n",
    "3.  Print the results of both calculations and use `np.allclose()` to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "k2l3m4n5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "l3m4n5o6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Eigenvalue (λ2): 4.0\n",
      "Second Eigenvector (v2): [ 0.4472136  -0.89442719]\n",
      "\n",
      "---\n",
      "B @ v2 = [ 1.78885438 -3.57770876]\n",
      "λ2 * v2 = [ 1.78885438 -3.57770876]\n",
      "\n",
      "Are they close? True\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "lambda2_B = eig_vals_B[1]\n",
    "v2_B = eig_vecs_B[:, 1]\n",
    "\n",
    "print(f\"Second Eigenvalue (λ2): {np.round(lambda2_B, 2)}\")\n",
    "print(f\"Second Eigenvector (v2): {v2_B}\\n\")\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"B @ v2 = {B @ v2_B}\")\n",
    "print(f\"λ2 * v2 = {lambda2_B * v2_B}\\n\")\n",
    "print(f\"Are they close? {np.allclose(B @ v2_B, lambda2_B * v2_B)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m4n5o6p7",
   "metadata": {},
   "source": [
    "## 4. Visualization and ML Connection (PCA)\n",
    "\n",
    "In Machine Learning, one of the most significant applications of eigenvectors is **Principal Component Analysis (PCA)**.\n",
    "\n",
    "PCA is a dimensionality reduction technique. It works by finding the directions of **maximum variance** in the data. These directions turn out to be the eigenvectors of the data's covariance matrix.\n",
    "\n",
    "-   The **eigenvector with the largest eigenvalue** is the **first principal component**. It is the direction in which the data varies the most.\n",
    "-   The eigenvector with the second-largest eigenvalue is the second principal component, and so on.\n",
    "\n",
    "By projecting the data onto a smaller number of principal components (the top eigenvectors), we can reduce the number of features while retaining the most important information (variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n5o6p7q8",
   "metadata": {},
   "source": [
    "### **Exercise 3: Conceptual Discussion**\n",
    "\n",
    "Consider a dataset with 10 features. You compute the covariance matrix and find its 10 eigenvalues. The first three eigenvalues are `[50.2, 25.1, 0.5]` and the remaining seven are all less than `0.1`.\n",
    "\n",
    "1.  What does the large magnitude of the first two eigenvalues tell you about the data?\n",
    "2.  If you were to use PCA to reduce the dimensionality of this dataset, how many principal components (eigenvectors) would you likely keep? Why?\n",
    "\n",
    "*Write your answer in the markdown cell below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9be95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "o6p7q8r9",
   "metadata": {},
   "source": [
    "*(Your answer for Exercise 3 here)*\n",
    "\n",
    "**Solution:**\n",
    "1. The large magnitude of the first two eigenvalues (50.2 and 25.1) indicates that most of the variance (spread) in the data lies along the directions of their corresponding eigenvectors (the first two principal components). The data is highly correlated and not just a random cloud of points.\n",
    "\n",
    "2. You would likely keep just **two** principal components. The first two eigenvalues account for the vast majority of the total variance (50.2 + 25.1 = 75.3), while the other eight eigenvalues are very small, meaning their corresponding eigenvectors represent directions with very little information (variance). By projecting the 10-dimensional data onto the first two principal components, you can reduce the dataset to 2 dimensions while preserving most of its important structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p7q8r9s0",
   "metadata": {},
   "source": [
    "## Day 7 Summary and Key Takeaways\n",
    "\n",
    "Great job today! Eigenvalues and eigenvectors are a deep topic, but understanding them from a practical standpoint is a huge step forward.\n",
    "\n",
    "Here's what we covered:\n",
    "-   **Eigenvectors** are vectors whose direction is unchanged by a linear transformation.\n",
    "-   **Eigenvalues (`λ`)** are the scalars by which eigenvectors are scaled during the transformation.\n",
    "-   The core relationship is **`Av = λv`**.\n",
    "-   NumPy's **`np.linalg.eig()`** is the essential tool for finding eigenvalues and eigenvectors.\n",
    "-   In Machine Learning, eigenvalues and eigenvectors are the foundation of **PCA**, where they help find the directions of maximum variance in the data for effective dimensionality reduction.\n",
    "\n",
    "Tomorrow, we'll look at another powerful matrix decomposition technique that is even more general: Singular Value Decomposition (SVD)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
