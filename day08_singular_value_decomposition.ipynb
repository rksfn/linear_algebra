{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Day 8: Singular Value Decomposition (SVD) - Introduction\n",
    "\n",
    "Welcome to Day 8! Today, we'll learn about one of the most powerful and versatile matrix decomposition techniques in linear algebra: **Singular Value Decomposition (SVD)**. SVD is more general than eigendecomposition because it works on *any* matrix, not just square ones. It is a cornerstone of many machine learning applications, from dimensionality reduction to recommender systems.\n",
    "\n",
    "## Objectives for Today:\n",
    "- Understand how SVD decomposes a matrix into three simpler matrices (U, Σ, Vᵀ).\n",
    "- See the relationship between SVD and eigenvalues for symmetric matrices.\n",
    "- Learn how to perform SVD on a matrix using NumPy.\n",
    "- Reconstruct a matrix from its SVD components.\n",
    "- Perform low-rank approximation by truncating singular values.\n",
    "- Connect SVD to its applications in ML, like image compression and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## 1. What is Singular Value Decomposition?\n",
    "\n",
    "SVD states that any matrix `A` (of shape `m x n`) can be factored into three separate matrices:\n",
    "\n",
    "### **`A = UΣVᵀ`**\n",
    "\n",
    "Where:\n",
    "-   **`A`** is the original `m x n` matrix.\n",
    "-   **`U`** is an `m x m` orthogonal matrix. Its columns are the **left-singular vectors**.\n",
    "-   **`Σ`** (Sigma) is an `m x n` diagonal matrix. Its diagonal entries are the **singular values** of `A`. These values are always non-negative and are ordered from largest to smallest.\n",
    "-   **`Vᵀ`** is the transpose of an `n x n` orthogonal matrix `V`. The columns of `V` (and thus rows of `Vᵀ`) are the **right-singular vectors**.\n",
    "\n",
    "**Key Idea:** SVD breaks down a complex transformation (matrix `A`) into a series of simpler transformations: a rotation (`Vᵀ`), a scaling (`Σ`), and another rotation (`U`). The singular values in `Σ` tell us the importance of each dimension in this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "## 2. SVD with NumPy\n",
    "\n",
    "NumPy's `np.linalg.svd()` function is the tool we use to perform SVD.\n",
    "\n",
    "It returns a tuple of three arrays: `U`, `s`, and `Vh`.\n",
    "-   `U` is the matrix of left-singular vectors.\n",
    "-   `s` is a **1D array** containing the singular values (the diagonal of `Σ`). It is *not* the full `Σ` matrix.\n",
    "-   `Vh` is the transposed matrix of right-singular vectors (`Vᵀ`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "---\n",
      "U:\n",
      " [[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "Shape of U: (3, 3)\n",
      "\n",
      "Singular values (s):\n",
      " [9.52551809 0.51430058]\n",
      "Shape of s: (2,)\n",
      "\n",
      "V transpose (Vh):\n",
      " [[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n",
      "Shape of Vh: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define a non-square matrix\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "\n",
    "# Perform SVD\n",
    "U, s, Vh = np.linalg.svd(A)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Shape of U:\", U.shape)\n",
    "\n",
    "print(\"\\nSingular values (s):\\n\", s)\n",
    "print(\"Shape of s:\", s.shape)\n",
    "\n",
    "print(\"\\nV transpose (Vh):\\n\", Vh)\n",
    "print(\"Shape of Vh:\", Vh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "### **Exercise 1: Reconstructing the Original Matrix**\n",
    "\n",
    "To reconstruct `A`, we need to convert the 1D array `s` into the `m x n` diagonal matrix `Σ`. Then we can multiply `U @ Σ @ Vh`.\n",
    "\n",
    "1.  Initialize a zero matrix `Sigma` with the same shape as the original matrix `A` (`3 x 2`).\n",
    "2.  Fill the diagonal of `Sigma` with the singular values from `s`.\n",
    "3.  Reconstruct `A` by multiplying the three matrices.\n",
    "4.  Use `np.allclose()` to check if the reconstructed matrix is close to the original `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "g7h8i9j0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma (Σ) matrix:\n",
      " [[9.52551809 0.        ]\n",
      " [0.         0.51430058]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Reconstructed Matrix:\n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "\n",
      "Is the original matrix similar to the reconstructed?\n",
      " True\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 1 here\n",
    "Sigma = np.zeros(A.shape)\n",
    "Sigma[:A.shape[1], :A.shape[1]] = np.diag(s)\n",
    "\n",
    "print(\"Sigma (Σ) matrix:\\n\", Sigma)\n",
    "\n",
    "A_reconstructed = U @ Sigma @Vh\n",
    "print(\"\\nReconstructed Matrix:\\n\", A_reconstructed)\n",
    "\n",
    "print(\"\\nIs the original matrix similar to the reconstructed?\\n\", np.allclose(A, A_reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8i9j0k1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma (Σ) matrix:\n",
      " [[9.52551809 0.        ]\n",
      " [0.         0.51430058]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Reconstructed Matrix:\n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "\n",
      "Is the reconstructed matrix close to the original? True\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# 1. Create a zero matrix with the correct shape\n",
    "Sigma = np.zeros(A.shape) # (3, 2)\n",
    "\n",
    "# 2. Fill the diagonal with singular values\n",
    "Sigma[:A.shape[1], :A.shape[1]] = np.diag(s) # Fill 2x2 part\n",
    "\n",
    "print(\"Sigma (Σ) matrix:\\n\", Sigma)\n",
    "\n",
    "# 3. Reconstruct A\n",
    "A_reconstructed = U @ Sigma @ Vh\n",
    "\n",
    "print(\"\\nReconstructed Matrix:\\n\", A_reconstructed)\n",
    "\n",
    "# 4. Check for closeness\n",
    "print(\"\\nIs the reconstructed matrix close to the original?\", np.allclose(A, A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 3. Low-Rank Approximation\n",
    "\n",
    "The magic of SVD is that the largest singular values correspond to the most significant \"parts\" of the matrix. We can create a **low-rank approximation** of the original matrix by keeping only the top `k` singular values.\n",
    "\n",
    "This is incredibly useful for compressing data, as we only need to store the first `k` columns of `U`, the first `k` singular values, and the first `k` rows of `Vh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "j0k1l2m3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix A:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Rank-1 Approximation of A:\n",
      " [[1.35662819 1.71846235]\n",
      " [3.09719707 3.92326845]\n",
      " [4.83776596 6.12807454]]\n"
     ]
    }
   ],
   "source": [
    "# Perform a rank-1 approximation (k=1)\n",
    "k = 1\n",
    "\n",
    "U_k = U[:, :k]         # First k columns of U\n",
    "s_k = s[:k]           # First k singular values\n",
    "Vh_k = Vh[:k, :]      # First k rows of Vh\n",
    "\n",
    "# Reconstruct the rank-1 matrix\n",
    "A_approx_k1 = U_k @ np.diag(s_k) @ Vh_k\n",
    "\n",
    "print(\"Original matrix A:\\n\", A)\n",
    "print(\"\\nRank-1 Approximation of A:\\n\", A_approx_k1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "### **Exercise 2: Perform a Low-Rank Approximation**\n",
    "\n",
    "1.  Define a new matrix `B`:\n",
    "    ```\n",
    "    B = np.array([,\n",
    "                  [-1, 3, 1]])\n",
    "    ```\n",
    "2.  Perform SVD on matrix `B`.\n",
    "3.  Create and print a **rank-1 approximation** of `B`.\n",
    "4.  Notice how the rank-1 approximation captures the general structure but loses some detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix B:\n",
      " [[ 9  2  5]\n",
      " [-1  3  1]]\n",
      "\n",
      "Rank-1 Approximation of B:\n",
      " [[8.97614585 2.05974141 5.01814744]\n",
      " [0.18126233 0.04159397 0.10133538]]\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 2 here\n",
    "B = np.array([[9, 2, 5], [-1, 3, 1]])\n",
    "\n",
    "U_b, s_b, Vh_b = np.linalg.svd(B)\n",
    "\n",
    "k = 1\n",
    "\n",
    "B_approx_k1 = U_b[:, :k] @ np.diag(s_b[:k]) @ Vh_b[:k, :]\n",
    "\n",
    "print(\"Original Matrix B:\\n\", B)\n",
    "print(\"\\nRank-1 Approximation of B:\\n\", B_approx_k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "m3n4o5p6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix B:\n",
      " [[ 3  1  1]\n",
      " [-1  3  1]]\n",
      "\n",
      "Rank-1 Approximation of B:\n",
      " [[1. 2. 1.]\n",
      " [1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "B = np.array([[3, 1, 1], [-1, 3, 1]])\n",
    "\n",
    "# Perform SVD\n",
    "U_b, s_b, Vh_b = np.linalg.svd(B)\n",
    "\n",
    "# Create rank-1 approximation\n",
    "k = 1\n",
    "B_approx_k1 = U_b[:, :k] @ np.diag(s_b[:k]) @ Vh_b[:k, :]\n",
    "\n",
    "print(\"Original Matrix B:\\n\", B)\n",
    "print(\"\\nRank-1 Approximation of B:\\n\", B_approx_k1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7",
   "metadata": {},
   "source": [
    "## 4. ML Connection: Compression, Recommenders, and more\n",
    "\n",
    "SVD is one of the most useful algorithms in machine learning.\n",
    "\n",
    "-   **Dimensionality Reduction & PCA:** SVD is the mathematical engine behind Principal Component Analysis. It finds the principal components by performing SVD on the centered data matrix.\n",
    "\n",
    "-   **Image Compression:** An image can be represented as a matrix of pixel values. A low-rank approximation of this matrix using SVD can capture the essential features of the image while using significantly less data to store.\n",
    "\n",
    "-   **Recommender Systems:** In a user-item matrix (where rows are users and columns are items they've rated), SVD can help find latent (hidden) factors. The low-rank approximation can predict missing ratings, forming the basis of matrix factorization techniques for recommendations.\n",
    "\n",
    "-   **Natural Language Processing (NLP):** SVD is used in Latent Semantic Analysis (LSA) to analyze relationships between documents and terms, helping to uncover underlying topics in a large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "## Day 8 Summary and Key Takeaways\n",
    "\n",
    "Excellent work! SVD is a fundamental technique that unlocks many advanced applications.\n",
    "\n",
    "Here's what we covered:\n",
    "-   SVD decomposes **any** matrix `A` into `UΣVᵀ`.\n",
    "-   `U` and `V` are orthogonal matrices (rotations), and `Σ` is a diagonal matrix of singular values (scaling).\n",
    "-   `np.linalg.svd()` is the function to use, but remember it returns the singular values as a 1D array `s`.\n",
    "-   **Low-rank approximation** is the key application, where we use the top `k` singular values to create a compressed version of the original matrix.\n",
    "-   This simple idea is the basis for powerful techniques in PCA, image compression, and recommender systems.\n",
    "\n",
    "Tomorrow, we'll dive into the more abstract but crucial concepts of vector spaces, subspaces, and the rank of a matrix."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
