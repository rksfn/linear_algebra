{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a1b2c3",
   "metadata": {},
   "source": [
    "# Day 10: Applications and Review\n",
    "\n",
    "Welcome to the final day of our 2-week linear algebra curriculum! Today, we bring everything together by implementing two cornerstone machine learning applications from scratch (or close to it) using NumPy. This will solidify your understanding of how abstract concepts like matrix inversion and eigenvectors become powerful, practical tools.\n",
    "\n",
    "## Objectives for Today:\n",
    "- Quickly review the key concepts from the past two weeks.\n",
    "- Understand and implement the Normal Equation for Linear Regression.\n",
    "- Apply Principal Component Analysis (PCA) to a dataset.\n",
    "- Connect the practical application of PCA back to the theory of eigenvalues and eigenvectors.\n",
    "- Consolidate your understanding of how linear algebra is the backbone of ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3d4e5",
   "metadata": {},
   "source": [
    "## 1. A Quick Review of Key Concepts\n",
    "\n",
    "Over the last nine days, we've built a solid foundation. Let's recap the journey:\n",
    "\n",
    "- **Week 1: Fundamentals**\n",
    "  - **Vectors & Matrices:** The basic data structures for features and datasets.\n",
    "  - **Core Operations:** Addition, multiplication, dot products, and transposes are the arithmetic of linear algebra.\n",
    "  - **Special Matrices:** The identity matrix and diagonal matrices are crucial for transformations and theory.\n",
    "\n",
    "- **Week 2: Advanced Concepts & Applications**\n",
    "  - **Matrix Inverse:** Key to solving systems of linear equations directly.\n",
    "  - **Eigenvectors & Eigenvalues:** Revealed the intrinsic directions and scaling factors of linear transformations.\n",
    "  - **SVD & Vector Spaces:** Provided tools for matrix decomposition and understanding the 'true' dimensionality of data.\n",
    "\n",
    "Today, we'll see how these pieces fit together to build real algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4e5f6",
   "metadata": {},
   "source": [
    "## 2. Application 1: Linear Regression with the Normal Equation\n",
    "\n",
    "Linear regression aims to find the best-fitting line (or hyperplane) through a set of data points. While iterative methods like gradient descent are common, there is a direct analytical solution known as the **Normal Equation**.\n",
    "\n",
    "The equation finds the optimal parameters `θ` (theta) that minimize the cost function:\n",
    "\n",
    "### `θ = (XᵀX)⁻¹Xᵀy`\n",
    "\n",
    "Look closely at the components! It's all operations we've learned:\n",
    "- `Xᵀ`: The transpose of our input feature matrix.\n",
    "- `XᵀX`: Matrix multiplication.\n",
    "- `(XᵀX)⁻¹`: The inverse of the resulting square matrix.\n",
    "\n",
    "This one equation elegantly combines many of the core concepts we've practiced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5f6g7",
   "metadata": {},
   "source": [
    "### **Exercise 1: Implement OLS Linear Regression from Scratch**\n",
    "\n",
    "1.  Create some sample 1D data `x` and `y`.\n",
    "2.  Construct the design matrix `X` by adding a column of ones for the intercept term.\n",
    "3.  Use the Normal Equation formula to calculate the optimal `theta` (which will contain the intercept and slope).\n",
    "4.  Plot the original data points and the regression line you calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create sample data\n",
    "np.random.seed(42) # for reproducibility\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1) # y = 4 + 3x + noise\n",
    "\n",
    "# 2. Construct the design matrix X (add x0 = 1 to each instance)\n",
    "# We use np.c_ which is a quick way to concatenate columns\n",
    "X = np.c_[np.ones((100, 1)), x]\n",
    "\n",
    "# 3. Calculate theta using the Normal Equation\n",
    "X_transpose = X.T\n",
    "XTX = X_transpose @ X\n",
    "XTX_inv = np.linalg.inv(XTX)\n",
    "XTy = X_transpose @ y\n",
    "theta = XTX_inv @ XTy\n",
    "\n",
    "print(\"Calculated Theta (intercept, slope):\\n\", theta)\n",
    "\n",
    "# 4. Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, alpha=0.6, label='Original Data')\n",
    "\n",
    "# To plot the line, we create two points and draw a line between them\n",
    "x_line = np.array([0, 2])\n",
    "X_line = np.c_[np.ones((2, 1)), x_line]\n",
    "y_line = X_line @ theta\n",
    "\n",
    "plt.plot(x_line, y_line, 'r-', label='Regression Line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression using the Normal Equation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70g7h8i9",
   "metadata": {},
   "source": [
    "The calculated `theta` should be very close to our original parameters of `[4, 3]`. This demonstrates the power of using linear algebra to solve an optimization problem in a single step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80h8i9j0",
   "metadata": {},
   "source": [
    "## 3. Application 2: Principal Component Analysis (PCA)\n",
    "\n",
    "PCA helps us find the directions of maximum variance in our data. As we discussed on Day 7, these directions are the **eigenvectors** of the data's covariance matrix. The magnitude of variance in these directions is given by the corresponding **eigenvalues**.\n",
    "\n",
    "While we could compute the covariance matrix and then find its eigenvectors manually, high-level libraries like `scikit-learn` provide a `PCA` object that does this for us. Our goal here is to use it and connect its output back to our linear algebra theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90i9j0k1",
   "metadata": {},
   "source": [
    "### **Exercise 2: Apply PCA and Visualize Components**\n",
    "\n",
    "1.  Generate a 2D dataset that is somewhat correlated.\n",
    "2.  Use `sklearn.decomposition.PCA` to find the two principal components.\n",
    "3.  Examine the `components_` and `explained_variance_` attributes of the fitted PCA object.\n",
    "4.  Plot the original data along with the principal components (eigenvectors) to visualize the directions of maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate correlated 2D data\n",
    "np.random.seed(42)\n",
    "X_pca, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n",
    "# Stretch and rotate the data to make it correlated\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_pca = X_pca @ transformation\n",
    "\n",
    "# 2. Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_pca)\n",
    "\n",
    "# 3. Examine the results and connect to linear algebra\n",
    "components = pca.components_\n",
    "explained_variance = pca.explained_variance_\n",
    "\n",
    "print(\"Principal Components (Eigenvectors):\\n\", components)\n",
    "print(\"\\nExplained Variance (Eigenvalues):\\n\", explained_variance)\n",
    "\n",
    "# 4. Plot the data and components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, label='Original Data')\n",
    "\n",
    "# Plot the eigenvectors (principal components)\n",
    "mean = pca.mean_ # Eigenvectors are centered at the mean of the data\n",
    "for i, vector in enumerate(components):\n",
    "    # Scale the vector by its eigenvalue for better visualization\n",
    "    v = vector * 3 * np.sqrt(explained_variance[i])\n",
    "    plt.arrow(mean[0], mean[1], v[0], v[1], head_width=0.1, head_length=0.2, linewidth=2, color='r', label=f'PC {i+1}')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('PCA: Data and Principal Components')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "# Adding a legend can be tricky with plt.arrow, this is a simple workaround\n",
    "plt.legend(['Original Data', 'Principal Components'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110k1l2m3",
   "metadata": {},
   "source": [
    "The plot clearly shows the two red arrows representing the eigenvectors. The longer arrow points in the direction of the highest variance (corresponding to the largest eigenvalue), which is the first principal component. This is a perfect geometric illustration of what eigen-decomposition does for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120l2m3n4",
   "metadata": {},
   "source": [
    "## Course Summary and Final Takeaways\n",
    "\n",
    "Congratulations! You've completed this two-week practical journey through linear algebra for machine learning.\n",
    "\n",
    "You have moved from the basic definitions of vectors and matrices to implementing fundamental ML applications. You now have a practical, code-first understanding of how:\n",
    "\n",
    "- **Data is represented** using vectors and matrices.\n",
    "- **Matrix operations** can solve complex problems like linear regression directly and efficiently.\n",
    "- **Eigen-decomposition** is not just an abstract theory but a powerful tool for understanding the structure of data and reducing its dimensionality (PCA).\n",
    "\n",
    "Linear algebra is the language of machine learning. With the foundation you've built, you are now much better equipped to understand the inner workings of more advanced algorithms and to confidently implement and debug your own models. Keep practicing and exploring!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
