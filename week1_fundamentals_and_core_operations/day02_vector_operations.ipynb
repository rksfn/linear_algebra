{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Day 2: Vector Operations\n",
    "\n",
    "Welcome to Day 2! Today, we'll build upon our understanding of vectors by exploring the fundamental operations that can be performed on them. These operations are crucial for manipulating data in machine learning algorithms.\n",
    "\n",
    "## Objectives for Today:\n",
    "- Understand and implement **vector addition**.\n",
    "- Understand and implement **scalar multiplication**.\n",
    "- Calculate **vector magnitude (L2 norm)**.\n",
    "- Understand and compute the **dot product** of two vectors.\n",
    "- Explore the concept of **orthogonal vectors**.\n",
    "- Connect these operations to their applications in Machine Learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "scrolled": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Vector Addition\n",
    "\n",
    "### Concept\n",
    "Vector addition involves adding two or more vectors component-wise. This means you add the corresponding elements of the vectors. For vector addition to be defined, the vectors must have the same dimension.\n",
    "\n",
    "Geometrically, if you place the tail of the second vector at the head of the first vector, the resultant vector (sum) is the vector from the tail of the first to the head of the second.\n",
    "\n",
    "If $u = [u_1, u_2]$ and $v = [v_1, v_2]$, then $u + v = [u_1 + v_1, u_2 + v_2]$.\n",
    "\n",
    "### NumPy Practice\n",
    "NumPy makes vector addition very straightforward using the `+` operator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([3, 4])\n",
    "\n",
    "v_sum = v1 + v2\n",
    "print(\"Vector 1:\", v1)\n",
    "print(\"Vector 2:\", v2)\n",
    "print(\"Sum of vectors:\", v_sum)\n",
    "\n",
    "# Geometric Visualization (Optional, but good for intuition)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\n",
    "plt.quiver(v1[0], v1[1], v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='g', label='v2 (from head of v1)')\n",
    "plt.quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1, color='b', label='v_sum', linestyle='--')\n",
    "\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 7)\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.grid()\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Vector Addition Geometrically\")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 1: Vector Addition**\n",
    "\n",
    "1.  Define two 3D vectors:\n",
    "    *   `vec_x = np.array([5, -1, 2])`\n",
    "    *   `vec_y = np.array([-2, 4, 3])`\n",
    "2.  Calculate their sum, `vec_z = vec_x + vec_y`.\n",
    "3.  Print `vec_x`, `vec_y`, and `vec_z`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 1 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 1\n",
    "vec_x = np.array([5, -1, 2])\n",
    "vec_y = np.array([-2, 4, 3])\n",
    "\n",
    "vec_z = vec_x + vec_y\n",
    "\n",
    "print(\"Vector X:\", vec_x)\n",
    "print(\"Vector Y:\", vec_y)\n",
    "print(\"Vector Z (Sum):\", vec_z)\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Scalar Multiplication\n",
    "\n",
    "### Concept\n",
    "Scalar multiplication involves multiplying a vector by a scalar (a single number). This operation scales the vector: it stretches it if the scalar is greater than 1, shrinks it if between 0 and 1, and reverses its direction if the scalar is negative.\n",
    "\n",
    "If $v = [v_1, v_2]$ and $c$ is a scalar, then $c \cdot v = [c \cdot v_1, c \cdot v_2]$.\n",
    "\n",
    "### NumPy Practice\n",
    "Similar to addition, NumPy handles scalar multiplication intuitively with the `*` operator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "v = np.array([2, 1])\n",
    "scalar = 3\n",
    "\n",
    "scaled_v = scalar * v\n",
    "print(\"Original Vector v:\", v)\n",
    "print(\"Scalar:\", scalar)\n",
    "print(\"Scaled Vector (3 * v):\", scaled_v)\n",
    "\n",
    "negative_scalar = -0.5\n",
    "scaled_v_neg = negative_scalar * v\n",
    "print(\"Scaled Vector (-0.5 * v):\", scaled_v_neg)\n",
    "\n",
    "# Geometric Visualization\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', label='v', alpha=0.6)\n",
    "plt.quiver(0, 0, scaled_v[0], scaled_v[1], angles='xy', scale_units='xy', scale=1, color='red', label='3 * v')\n",
    "plt.quiver(0, 0, scaled_v_neg[0], scaled_v_neg[1], angles='xy', scale_units='xy', scale=1, color='green', label='-0.5 * v')\n",
    "\n",
    "plt.xlim(-3, 7)\n",
    "plt.ylim(-3, 4)\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.grid()\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Scalar Multiplication Geometrically\")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2: Scalar Multiplication and Combination**\n",
    "\n",
    "1.  Define a vector `data_point = np.array([10, 20, 30])` (e.g., features: [age, income, spending]).\n",
    "2.  Imagine you want to normalize this data point by dividing each feature by 100. Perform this operation using scalar multiplication and store it in `normalized_data_point`.\n",
    "3.  Now, create a new vector `target_vector = np.array([2, -1, 0.5])`.\n",
    "4.  Calculate `resultant_vector = 2 * data_point - 3 * target_vector`.\n",
    "5.  Print all resulting vectors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 2 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 2\n",
    "data_point = np.array([10, 20, 30])\n",
    "\n",
    "# Normalize by dividing by 100 (which is multiplying by 1/100 or 0.01)\n",
    "normalized_data_point = 0.01 * data_point\n",
    "print(\"Original data_point:\", data_point)\n",
    "print(\"Normalized data_point:\", normalized_data_point)\n",
    "\n",
    "target_vector = np.array([2, -1, 0.5])\n",
    "resultant_vector = 2 * data_point - 3 * target_vector\n",
    "\n",
    "print(\"\\nTarget Vector:\", target_vector)\n",
    "print(\"Resultant Vector (2 * data_point - 3 * target_vector):\", resultant_vector)\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Vector Magnitude (L2 Norm)\n",
    "\n",
    "### Concept\n",
    "The magnitude (or length, or Euclidean norm, or L2 norm) of a vector represents its length from the origin to its point in space. For a 2D vector $v = [v_1, v_2]$, the magnitude is $\\|v\\| = \\sqrt{v_1^2 + v_2^2}$. For an n-dimensional vector, it generalizes to $\\|v\\| = \\sqrt{\\sum_{i=1}^n v_i^2}$.\n",
    "\n",
    "### NumPy Practice\n",
    "NumPy's `np.linalg.norm()` function is used to calculate the magnitude."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "v_magnitude_example = np.array([3, 4])\n",
    "magnitude = np.linalg.norm(v_magnitude_example)\n",
    "\n",
    "print(\"Vector:\", v_magnitude_example)\n",
    "print(\"Magnitude (L2 Norm):\", magnitude)\n",
    "\n",
    "v_3d = np.array([1, 2, 2])\n",
    "magnitude_3d = np.linalg.norm(v_3d)\n",
    "print(\"\\n3D Vector:\", v_3d)\n",
    "print(\"Magnitude of 3D Vector:\", magnitude_3d)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 3: Calculating Vector Magnitude**\n",
    "\n",
    "1.  Create a vector `feature_vector = np.array([7, -24])`.\n",
    "2.  Calculate its L2 norm (magnitude).\n",
    "3.  Create another vector `random_vector = np.array([1.5, -0.5, 3.0, 2.0])`.\n",
    "4.  Calculate its magnitude.\n",
    "5.  Print both vectors and their magnitudes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 3 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 3\n",
    "feature_vector = np.array([7, -24])\n",
    "magnitude_feature = np.linalg.norm(feature_vector)\n",
    "print(\"Feature Vector:\", feature_vector)\n",
    "print(\"Magnitude of Feature Vector:\", magnitude_feature)\n",
    "\n",
    "random_vector = np.array([1.5, -0.5, 3.0, 2.0])\n",
    "magnitude_random = np.linalg.norm(random_vector)\n",
    "print(\"\\nRandom Vector:\", random_vector)\n",
    "print(\"Magnitude of Random Vector:\", magnitude_random)\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Dot Product\n",
    "\n",
    "### Concept\n",
    "The dot product (also known as the scalar product or inner product) takes two vectors and returns a single scalar value. It's a measure of how much two vectors point in the same direction.\n",
    "\n",
    "For $u = [u_1, u_2]$ and $v = [v_1, v_2]$, the dot product is $u \cdot v = u_1v_1 + u_2v_2$.\n",
    "For higher dimensions: $u \cdot v = \\sum_{i=1}^n u_i v_i$.\n",
    "\n",
    "Key interpretations:\n",
    "-   If the dot product is positive, the vectors generally point in the same direction (angle < 90 degrees).\n",
    "-   If the dot product is zero, the vectors are orthogonal (perpendicular, angle = 90 degrees).\n",
    "-   If the dot product is negative, the vectors generally point in opposite directions (angle > 90 degrees).\n",
    "\n",
    "It can also be defined as $u \\cdot v = \\|u\\| \\|v\\| \\cos(\\theta)$, where $\\theta$ is the angle between the vectors. This formulation is used to calculate cosine similarity.\n",
    "\n",
    "### NumPy Practice\n",
    "NumPy provides `np.dot()` or the `@` operator for the dot product."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "u = np.array([1, 2])\n",
    "v = np.array([3, 1])\n",
    "\n",
    "dot_product_uv = np.dot(u, v)\n",
    "print(\"Vector u:\", u)\n",
    "print(\"Vector v:\", v)\n",
    "print(\"Dot product (u . v):\", dot_product_uv)\n",
    "\n",
    "# Using the @ operator (Python 3.5+)\n",
    "dot_product_uv_at = u @ v\n",
    "print(\"Dot product (u @ v):\", dot_product_uv_at)\n",
    "\n",
    "# Another example for 3D vectors\n",
    "a = np.array([1, 0, 0])\n",
    "b = np.array([0, 1, 0])\n",
    "print(\"\\nVector a:\", a)\n",
    "print(\"Vector b:\", b)\n",
    "print(\"Dot product (a . b):\", np.dot(a, b)) # Should be 0, indicating orthogonality\n",
    "\n",
    "c = np.array([1, 1])\n",
    "d = np.array([-1, -1])\n",
    "print(\"\\nVector c:\", c)\n",
    "print(\"Vector d:\", d)\n",
    "print(\"Dot product (c . d):\", np.dot(c, d)) # Should be negative, opposite directions"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 4: Dot Product and Interpretation**\n",
    "\n",
    "1.  Define two feature vectors:\n",
    "    *   `f1 = np.array([10, 5, 2])` (e.g., [feature_a, feature_b, feature_c])\n",
    "    *   `f2 = np.array([3, 1, 4])` (e.g., [weights_a, weights_b, weights_c])\n",
    "2.  Calculate their dot product.\n",
    "3.  Based on the result, briefly explain what this dot product value implies about the relationship between `f1` and `f2` (e.g., are they generally pointing in the same direction?).\n",
    "4.  Consider a sentiment analysis scenario. You have a `word_vector = np.array([0.8, -0.2, 0.1])` representing a word. You also have a `sentiment_score_vector = np.array([0.5, 0.5, -0.5])` where each component indicates a positive, neutral, or negative contribution.\n",
    "5.  Calculate the dot product of `word_vector` and `sentiment_score_vector`. What might this result indicate about the word's overall sentiment?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 4 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 4\n",
    "f1 = np.array([10, 5, 2])\n",
    "f2 = np.array([3, 1, 4])\n",
    "\n",
    "dot_f1_f2 = np.dot(f1, f2)\n",
    "print(\"f1:\", f1)\n",
    "print(\"f2:\", f2)\n",
    "print(\"Dot product (f1 . f2):\", dot_f1_f2)\n",
    "print(\"Interpretation: Since the dot product is positive, vectors f1 and f2 generally point in the same direction. This implies a positive correlation or alignment between their components.\")\n",
    "\n",
    "word_vector = np.array([0.8, -0.2, 0.1])\n",
    "sentiment_score_vector = np.array([0.5, 0.5, -0.5])\n",
    "\n",
    "sentiment_dot_product = np.dot(word_vector, sentiment_score_vector)\n",
    "print(\"\\nWord Vector:\", word_vector)\n",
    "print(\"Sentiment Score Vector:\", sentiment_score_vector)\n",
    "print(\"Dot product (Word . Sentiment):\", sentiment_dot_product)\n",
    "print(\"Interpretation: The positive dot product ({:.2f}) suggests a generally positive sentiment for the word, as its components align more with the positive contributions in the sentiment score vector. A higher positive value would indicate stronger positive sentiment, while a negative value would imply negative sentiment.\".format(sentiment_dot_product))\n".format(sentiment_dot_product))\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Orthogonal Vectors\n",
    "\n",
    "### Concept\n",
    "Two vectors are considered **orthogonal** (or perpendicular) if the angle between them is 90 degrees. Mathematically, this means their dot product is zero.\n",
    "\n",
    "$u \cdot v = 0 \\implies u \\perp v$\n",
    "\n",
    "This concept is extremely important in many areas of machine learning, such as PCA and basis transformations.\n",
    "\n",
    "### NumPy Practice\n",
    "We can simply calculate the dot product and check if it's close to zero (due to floating-point precision, an exact zero might be rare)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vec1 = np.array([2, 1])\n",
    "vec2 = np.array([-1, 2])\n",
    "\n",
    "dot_prod_ortho = np.dot(vec1, vec2)\n",
    "\n",
    "print(\"Vector 1:\", vec1)\n",
    "print(\"Vector 2:\", vec2)\n",
    "print(\"Dot product:\", dot_prod_ortho)\n",
    "\n",
    "if np.isclose(dot_prod_ortho, 0):\n",
    "    print(\"Vectors are orthogonal (or very close to it).\")\n",
    "else:\n",
    "    print(\"Vectors are not orthogonal.\")\n",
    "\n",
    "# Geometric Visualization\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.quiver(0, 0, vec1[0], vec1[1], angles='xy', scale_units='xy', scale=1, color='purple', label='vec1')\n",
    "plt.quiver(0, 0, vec2[0], vec2[1], angles='xy', scale_units='xy', scale=1, color='orange', label='vec2')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.grid()\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.title(\"Orthogonal Vectors\")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5: Checking for Orthogonality**\n",
    "\n",
    "1.  Define `v_a = np.array([3, -2, 1])`.\n",
    "2.  Define `v_b = np.array([1, 2, 1])`.\n",
    "3.  Calculate the dot product of `v_a` and `v_b`.\n",
    "4.  Determine if `v_a` and `v_b` are orthogonal.\n",
    "5.  Find a new vector `v_c` (e.g., `np.array([-1, -1, 5])`) and check if `v_a` and `v_c` are orthogonal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Your code for Exercise 5 here\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Solution for Exercise 5\n",
    "v_a = np.array([3, -2, 1])\n",
    "v_b = np.array([1, 2, 1])\n",
    "\n",
    "dot_ab = np.dot(v_a, v_b)\n",
    "print(\"v_a:\", v_a)\n",
    "print(\"v_b:\", v_b)\n",
    "print(\"Dot product (v_a . v_b):\", dot_ab)\n",
    "if np.isclose(dot_ab, 0):\n",
    "    print(\"v_a and v_b are orthogonal.\")\n",
    "else:\n",
    "    print(\"v_a and v_b are NOT orthogonal.\")\n",
    "\n",
    "v_c = np.array([-1, -1, 5]) # This is chosen to be orthogonal to v_a\n",
    "dot_ac = np.dot(v_a, v_c)\n",
    "print(\"\\n--- Checking v_a and v_c ---\")\n",
    "print(\"v_a:\", v_a)\n",
    "print(\"v_c:\", v_c)\n",
    "print(\"Dot product (v_a . v_c):\", dot_ac)\n",
    "if np.isclose(dot_ac, 0):\n",
    "    print(\"v_a and v_c are orthogonal.\")\n",
    "else:\n",
    "    print(\"v_a and v_c are NOT orthogonal.\")\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Day 2 Summary and ML Connection\n",
    "\n",
    "Today, we covered the core operations that make vectors so powerful in linear algebra and machine learning:\n",
    "\n",
    "-   **Vector Addition:** Combining two vectors component-wise. In ML, this could represent combining different influences or aggregating features.\n",
    "-   **Scalar Multiplication:** Scaling a vector by a constant. Used in feature scaling (e.g., normalizing pixel values) or adjusting weights in models.\n",
    "-   **Vector Magnitude (L2 Norm):** The length of a vector. Important for measuring distance (e.g., Euclidean distance between data points in clustering algorithms like K-Means) or model complexity (regularization terms).\n",
    "-   **Dot Product:** A scalar value indicating the alignment of two vectors. Absolutely crucial for:\n",
    "    -   **Similarity measures:** Like cosine similarity, used in recommender systems or natural language processing.\n",
    "    -   **Projections:** Decomposing vectors into components along certain directions.\n",
    "    -   **Neural Networks:** The core operation in a neuron, where input features are multiplied by weights and summed up (`w . x + b`).\n",
    "-   **Orthogonal Vectors:** Vectors whose dot product is zero, meaning they are perpendicular. This concept is vital for finding independent components in data (e.g., Principal Component Analysis) and constructing orthogonal bases.\n",
    "\n",
    "These operations are the building blocks for more complex linear algebra concepts that underpin almost every machine learning algorithm. Keep practicing, and you'll see how these abstract concepts translate into practical tools for data science!"
   ],
   "metadata": {}
  }
 ]
}
